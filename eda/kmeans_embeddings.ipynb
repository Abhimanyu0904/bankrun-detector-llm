{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting scikit-learn\n",
      "  Obtaining dependency information for scikit-learn from https://files.pythonhosted.org/packages/18/ff/919087b9aa5318c3992def31b74e019eee39c57ff6bf140289041df3db86/scikit_learn-1.4.1.post1-cp311-cp311-macosx_12_0_arm64.whl.metadata\n",
      "  Downloading scikit_learn-1.4.1.post1-cp311-cp311-macosx_12_0_arm64.whl.metadata (11 kB)\n",
      "Requirement already satisfied: numpy<2.0,>=1.19.5 in /Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages (from scikit-learn) (1.26.3)\n",
      "Requirement already satisfied: scipy>=1.6.0 in /Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages (from scikit-learn) (1.12.0)\n",
      "Collecting joblib>=1.2.0 (from scikit-learn)\n",
      "  Obtaining dependency information for joblib>=1.2.0 from https://files.pythonhosted.org/packages/10/40/d551139c85db202f1f384ba8bcf96aca2f329440a844f924c8a0040b6d02/joblib-1.3.2-py3-none-any.whl.metadata\n",
      "  Downloading joblib-1.3.2-py3-none-any.whl.metadata (5.4 kB)\n",
      "Collecting threadpoolctl>=2.0.0 (from scikit-learn)\n",
      "  Obtaining dependency information for threadpoolctl>=2.0.0 from https://files.pythonhosted.org/packages/1e/84/ccd9b08653022b7785b6e3ee070ffb2825841e0dc119be22f0840b2b35cb/threadpoolctl-3.4.0-py3-none-any.whl.metadata\n",
      "  Downloading threadpoolctl-3.4.0-py3-none-any.whl.metadata (13 kB)\n",
      "Downloading scikit_learn-1.4.1.post1-cp311-cp311-macosx_12_0_arm64.whl (10.4 MB)\n",
      "\u001b[2K   \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m10.4/10.4 MB\u001b[0m \u001b[31m28.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m0:01\u001b[0m\n",
      "\u001b[?25hDownloading joblib-1.3.2-py3-none-any.whl (302 kB)\n",
      "\u001b[2K   \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m302.2/302.2 kB\u001b[0m \u001b[31m20.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading threadpoolctl-3.4.0-py3-none-any.whl (17 kB)\n",
      "Installing collected packages: threadpoolctl, joblib, scikit-learn\n",
      "Successfully installed joblib-1.3.2 scikit-learn-1.4.1.post1 threadpoolctl-3.4.0\n",
      "\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m23.2.1\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m24.0\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpip3 install --upgrade pip\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "!pip3 install scikit-learn\n",
    "\n",
    "import numpy as np\n",
    "import json\n",
    "from sklearn.cluster import KMeans"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Listing out tweets to cluster.\n",
    "base_path = \"/Users/shivamenta/Desktop/embeddings_twitter_data/\"\n",
    "banks = [\"all_risk.json\"]\n",
    "tweets = []\n",
    "clustered_tweets = {}\n",
    "\n",
    "for bank in banks:\n",
    "    filepath = base_path + bank\n",
    "    with open(filepath, \"r\") as f:\n",
    "        tweets.extend(json.loads(s) for s in f.readlines())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "def k_means_tweets(tweets, num_centroids=3, tweets_per_centroid=3):\n",
    "    global clustered_tweets\n",
    "    tweets_per_centroid = max(3, min(tweets_per_centroid, 5))\n",
    "    \n",
    "    embeddings = [tweet['embedding'] for tweet in tweets]\n",
    "    embeddings = np.array(embeddings)\n",
    "    \n",
    "    kmeans = KMeans(n_clusters=num_centroids, random_state=0).fit(embeddings)\n",
    "    tweet_labels = kmeans.labels_\n",
    "    \n",
    "    clustered_tweets = {}\n",
    "    for label, tweet in zip(tweet_labels, tweets):\n",
    "        if label not in clustered_tweets:\n",
    "            clustered_tweets[label] = []\n",
    "        clustered_tweets[label].append(tweet)\n",
    "    \n",
    "    for centroid_index, centroid in enumerate(kmeans.cluster_centers_):\n",
    "        print(f\"\\nCentroid {centroid_index}:\")\n",
    "        distances = []\n",
    "        for tweet in clustered_tweets[centroid_index]:\n",
    "            distance = np.linalg.norm(centroid - np.array(tweet['embedding']))\n",
    "            distances.append((tweet, distance))\n",
    "        \n",
    "        closest_tweets = sorted(distances, key=lambda x: x[1])[:tweets_per_centroid]\n",
    "        \n",
    "        for i, (tweet, distance) in enumerate(closest_tweets, start=1):\n",
    "            print(f\"{i}. {tweet['text']} (Distance: {distance:.2f})\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Centroid 0:\n",
      "1. SunTrust Bank Settles with Feds for Nearly $1 Billion http://t.co/reJxKMfKhi via @DSNewsDaily (Distance: 0.67)\n",
      "2. Suntrust Reaches $320 Million Settlement http://t.co/lmkH68UBon | by @RobertSiegel1 (Distance: 0.67)\n",
      "3. SEC Charges Fifth Third Bancorp and Former CFO for Improper Accounting of Loan Losses During Financial Crisis http://t.co/nmjF19DUzA (Distance: 0.68)\n",
      "4. (#On_Swager_1) SunTrust Mortgage Settles Federal Probe for $320M: SunTrust Mortgage to pay up t... http://t.co/JnFdnE1KZ6 (#On_Swager_1) (Distance: 0.71)\n",
      "5. SunTrust Bank, U.S. reach settlement in discrimination case: WASHINGTON (Reuters) - SunTrust Bank Inc's mortgage... http://t.co/mKP2YUl2 (Distance: 0.72)\n",
      "\n",
      "Centroid 1:\n",
      "1. ok screw Huntington bank and how slow they are #pissedaf (Distance: 0.60)\n",
      "2. Huntington bank is pissing me off (Distance: 0.61)\n",
      "3. Huntington Bank was about to have me flip out! (Distance: 0.65)\n",
      "4. swear i hate huntington bank ! ‚úãüòí (Distance: 0.65)\n",
      "5. I liked a @YouTube video from @spl1tsecsh00ter http://t.co/aaxf18u0pV Huntington Bank is a JOKE (Distance: 0.65)\n",
      "\n",
      "Centroid 2:\n",
      "1. RT @noticel: Doral Financial se lleva sus activos a Estados Unidos - http://t.co/oEbR5XHvdS (Distance: 0.58)\n",
      "2. Degradan a Doral Bank http://t.co/qDXIaK1E (via @elnuevodia) (Distance: 0.61)\n",
      "3. RT @lherrero: Doral Bank podr√≠a ser expulsado de la Bolsa de Nueva York - El Nuevo D√≠a http://t.co/U0w72849Yk via @elnuevodia (Distance: 0.61)\n",
      "4. RT @Metro_PR: Investigar√° la C√°mara transacci√≥n que beneficia a Doral Bank http://t.co/peht5Bu4tq (Distance: 0.63)\n",
      "5. RT @Metro_PR: Investigar√° la C√°mara transacci√≥n que beneficia a Doral Bank http://t.co/peht5Bu4tq (Distance: 0.63)\n",
      "\n",
      "Centroid 3:\n",
      "1. Bearish on these #stocks $HBAN $MLP $CRFN $NSM Visit http://t.co/k0tOQYkWQQ (Distance: 0.64)\n",
      "2. $HBAN - Huntington Bancsharesorporated Stock Analysis - RSI is bearish and falling - http://t.co/pNqgWMbi (Distance: 0.66)\n",
      "3. Rosner: $HBAN Overall Average: 56% Sell. $ATML $DRYS $HBAN http://t.co/bUHvXO7XQw (Distance: 0.66)\n",
      "4. $HBAN - [$$] Huntington Bancshares Earnings Drop Slightly -&gt; http://t.co/XBbFbqdjWF #stock #stocks #stockaction (Distance: 0.67)\n",
      "5. Bad #Invesments like got you down $FOXA $HBAN $SLGN $CSIQ #NASDAQ http://t.co/8cLqih4pZl (Distance: 0.68)\n"
     ]
    }
   ],
   "source": [
    "# Running kmeans.\n",
    "k_means_tweets(tweets, num_centroids=4, tweets_per_centroid=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Writing these tweets to disk.\n",
    "all_tweets = []\n",
    "for label in clustered_tweets:\n",
    "    for tweet in clustered_tweets[label]:\n",
    "        if \"embedding\" in tweet:\n",
    "            tweet.pop(\"embedding\")\n",
    "        tweet[\"centroid_label\"] = int(label)\n",
    "        all_tweets.append(tweet)\n",
    "\n",
    "with open(\"/Users/shivamenta/Desktop/labeled_data/risk_embeddings_labeled.json\", \"w\") as f:\n",
    "    json.dump(all_tweets, f)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
